"""
MÃ³dulo de treinamento para modelos de classificaÃ§Ã£o de lesÃµes
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import wandb
from tqdm import tqdm
import time


def format_time(seconds):
    """Formata tempo em segundos para formato legÃ­vel"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        minutes = int(seconds // 60)
        seconds = seconds % 60
        return f"{minutes}m {seconds:.1f}s"
    else:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        seconds = seconds % 60
        return f"{hours}h {minutes}m {seconds:.1f}s"


def train_epoch(model, train_loader, criterion, optimizer, device, epoch):
    """Training de uma Ã©poca com indicadores visuais detalhados"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    # Configurar barra de progresso
    num_batches = len(train_loader)
    pbar = tqdm(
        train_loader, 
        desc=f"Epoch {epoch+1} [TRAIN]", 
        ncols=120,
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
    )
    
    batch_times = []
    start_time = time.time()
    
    for batch_idx, (images, labels, _) in enumerate(pbar):
        batch_start = time.time()
        
        # Forward pass
        images = images.to(device)
        labels = labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # MÃ©tricas
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # Calcular tempo do batch
        batch_time = time.time() - batch_start
        batch_times.append(batch_time)
        
        # Calcular mÃ©tricas atuais
        current_loss = total_loss / (batch_idx + 1)
        current_acc = 100. * correct / total
        
        # Atualizar barra de progresso com informaÃ§Ãµes detalhadas
        pbar.set_postfix({
            'Loss': f'{current_loss:.4f}',
            'Acc': f'{current_acc:.2f}%',
            'Batch': f'{batch_idx+1}/{num_batches}',
            'Time': f'{batch_time:.3f}s'
        })
        
        # Log para wandb a cada 10 batches
        if batch_idx % 10 == 0:
            wandb.log({
                "batch_loss": loss.item(),
                "batch_accuracy": 100. * correct / total,
                "batch_time": batch_time
            })
    
    epoch_time = time.time() - start_time
    epoch_loss = total_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    
    # EstatÃ­sticas finais da Ã©poca
    avg_batch_time = sum(batch_times) / len(batch_times)
    print(f"\nğŸ“Š Epoch {epoch+1} [TRAIN] - Resumo:")
    print(f"   â±ï¸  Tempo total: {format_time(epoch_time)}")
    print(f"   ğŸ“ˆ Loss mÃ©dio: {epoch_loss:.4f}")
    print(f"   ğŸ¯ AcurÃ¡cia: {epoch_acc:.2f}%")
    print(f"   âš¡ Tempo mÃ©dio por batch: {avg_batch_time:.3f}s")
    print(f"   ğŸ“¦ Total de batches: {num_batches}")
    
    return epoch_loss, epoch_acc


def validate_epoch(model, val_loader, criterion, device, epoch):
    """ValidaÃ§Ã£o de uma Ã©poca com indicadores visuais"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    # Configurar barra de progresso para validaÃ§Ã£o
    pbar = tqdm(
        val_loader, 
        desc=f"Epoch {epoch+1} [VAL]", 
        ncols=120,
        bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
    )
    
    start_time = time.time()
    
    with torch.no_grad():
        for batch_idx, (images, labels, _) in enumerate(pbar):
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # Calcular mÃ©tricas atuais
            current_loss = total_loss / (batch_idx + 1)
            current_acc = 100. * correct / total
            
            # Atualizar barra de progresso
            pbar.set_postfix({
                'Loss': f'{current_loss:.4f}',
                'Acc': f'{current_acc:.2f}%',
                'Batch': f'{batch_idx+1}/{len(val_loader)}'
            })
    
    epoch_time = time.time() - start_time
    epoch_loss = total_loss / len(val_loader)
    epoch_acc = 100. * correct / total
    
    # EstatÃ­sticas finais da validaÃ§Ã£o
    print(f"ğŸ“Š Epoch {epoch+1} [VAL] - Resumo:")
    print(f"   â±ï¸  Tempo total: {format_time(epoch_time)}")
    print(f"   ğŸ“ˆ Loss mÃ©dio: {epoch_loss:.4f}")
    print(f"   ğŸ¯ AcurÃ¡cia: {epoch_acc:.2f}%")
    
    return epoch_loss, epoch_acc


def train_model(model, train_loader, val_loader, num_epochs, device, save_folder, 
                learning_rate=1e-5, weight_decay=0.01, patience=5):
    """
    FunÃ§Ã£o principal de treinamento com indicadores visuais aprimorados
    
    Args:
        model: Modelo PyTorch
        train_loader: DataLoader para treinamento
        val_loader: DataLoader para validaÃ§Ã£o
        num_epochs: NÃºmero de Ã©pocas
        device: Dispositivo (CPU/GPU)
        save_folder: Pasta para salvar checkpoints
        learning_rate: Taxa de aprendizado
        weight_decay: Decay do peso
        patience: PaciÃªncia para reduÃ§Ã£o de LR
    
    Returns:
        dict: DicionÃ¡rio com histÃ³rico de treinamento
    """
    
    # Loss function e optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=0.5)
    
    # Log do modelo para wandb
    wandb.watch(model, log="all")
    
    # InformaÃ§Ãµes iniciais
    print("ğŸš€ Iniciando training...")
    print("=" * 80)
    print(f"ğŸ“Š ConfiguraÃ§Ãµes:")
    print(f"   ğŸ¯ Total de Ã©pocas: {num_epochs}")
    print(f"   ğŸ“š Dados de treino: {len(train_loader.dataset)} imagens")
    print(f"   ğŸ” Dados de validaÃ§Ã£o: {len(val_loader.dataset)} imagens")
    print(f"   ğŸ“¦ Batch size: {train_loader.batch_size}")
    print(f"   ğŸ§  Learning rate: {learning_rate}")
    print(f"   âš–ï¸  Weight decay: {weight_decay}")
    print(f"   ğŸ’¾ Pasta de salvamento: {save_folder}")
    print(f"   ğŸ”§ Dispositivo: {device}")
    print("=" * 80)
    
    best_val_acc = 0.0
    training_history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': [],
        'learning_rate': []
    }
    
    # EstatÃ­sticas de tempo
    total_start_time = time.time()
    epoch_times = []
    
    for epoch in range(num_epochs):
        epoch_start_time = time.time()
        
        print(f"\n{'='*20} EPOCH {epoch+1}/{num_epochs} {'='*20}")
        print(f"â° Iniciando em: {time.strftime('%H:%M:%S')}")
        
        # Training
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, epoch)
        
        # Validation
        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device, epoch)
        
        # Learning rate scheduling
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)
        current_lr = optimizer.param_groups[0]['lr']
        
        # Log para wandb
        wandb.log({
            "epoch": epoch,
            "train_loss": train_loss,
            "train_accuracy": train_acc,
            "val_loss": val_loss,
            "val_accuracy": val_acc,
            "learning_rate": current_lr
        })
        
        # Salvar histÃ³rico
        training_history['train_loss'].append(train_loss)
        training_history['train_acc'].append(train_acc)
        training_history['val_loss'].append(val_loss)
        training_history['val_acc'].append(val_acc)
        training_history['learning_rate'].append(current_lr)
        
        # Calcular tempo da Ã©poca
        epoch_time = time.time() - epoch_start_time
        epoch_times.append(epoch_time)
        avg_epoch_time = sum(epoch_times) / len(epoch_times)
        remaining_epochs = num_epochs - (epoch + 1)
        estimated_remaining = remaining_epochs * avg_epoch_time
        
        # Resumo da Ã©poca
        print(f"\nğŸ¯ EPOCH {epoch+1} - Resumo Final:")
        print(f"   ğŸ“ˆ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"   ğŸ” Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
        print(f"   ğŸ§  Learning Rate: {current_lr:.6f}")
        if old_lr != current_lr:
            print(f"   âš¡ LR reduzido de {old_lr:.6f} para {current_lr:.6f}")
        
        print(f"   â±ï¸  Tempo da Ã©poca: {format_time(epoch_time)}")
        print(f"   â±ï¸  Tempo mÃ©dio por Ã©poca: {format_time(avg_epoch_time)}")
        print(f"   â±ï¸  Tempo restante estimado: {format_time(estimated_remaining)}")
        
        # Salvar melhor modelo
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            checkpoint_path = os.path.join(save_folder, 'best_model.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'training_history': training_history
            }, checkpoint_path)
            
            # Log do melhor modelo para wandb
            wandb.run.summary["best_val_accuracy"] = val_acc
            wandb.run.summary["best_epoch"] = epoch
            
            print(f"   ğŸ† ğŸ†• NOVO MELHOR MODELO! Val Acc: {val_acc:.2f}%")
            print(f"   ğŸ’¾ Modelo salvo em: {checkpoint_path}")
        else:
            print(f"   ğŸ† Melhor val_acc atÃ© agora: {best_val_acc:.2f}%")
        
        print(f"{'='*60}")
    
    # EstatÃ­sticas finais
    total_time = time.time() - total_start_time
    print(f"\nğŸ‰ TRAINING CONCLUÃDO!")
    print("=" * 80)
    print(f"ğŸ“Š EstatÃ­sticas Finais:")
    print(f"   ğŸ† Melhor val_acc: {best_val_acc:.2f}%")
    print(f"   â±ï¸  Tempo total: {format_time(total_time)}")
    print(f"   â±ï¸  Tempo mÃ©dio por Ã©poca: {format_time(total_time/num_epochs)}")
    print(f"   ğŸ“ˆ Loss final (train/val): {training_history['train_loss'][-1]:.4f}/{training_history['val_loss'][-1]:.4f}")
    print(f"   ğŸ¯ Acc final (train/val): {training_history['train_acc'][-1]:.2f}%/{training_history['val_acc'][-1]:.2f}%")
    print(f"   ğŸ’¾ Checkpoint salvo em: {os.path.join(save_folder, 'best_model.pth')}")
    print("=" * 80)
    
    return {
        'best_val_acc': best_val_acc,
        'training_history': training_history,
        'checkpoint_path': os.path.join(save_folder, 'best_model.pth')
    }
